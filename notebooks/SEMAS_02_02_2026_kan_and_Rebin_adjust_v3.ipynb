{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CouOWosICr1Q"
      },
      "source": [
        "# SEMAS-COMPLETE: FIXED METRICS - ADAPTIVE THRESHOLD & PROPER BEARING LABELS\n",
        "## Issue: Bearing dataset returns continuous RUL values, not binary labels (0/1)\n",
        "## Fix: Convert RUL â‰¤ 25 to anomaly (1), RUL > 25 to normal (0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ULETMBd7Cr1S",
        "outputId": "730b3ee7-52f4-4539-8d97-12add1679fa7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m38.0/38.0 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m40.1/40.1 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m142.0/142.0 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m55.3/55.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.7/12.7 MB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m154.9/154.9 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m441.4/441.4 kB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m125.1/125.1 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.7/24.7 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m80.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m525.6/525.6 kB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m87.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m108.7/108.7 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m117.2/117.2 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m312.3/312.3 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m756.0/756.0 kB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m55.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m90.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m140.1/140.1 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m67.2/67.2 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m948.6/948.6 kB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m779.2/779.2 kB\u001b[0m \u001b[31m51.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m95.5/95.5 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m81.5/81.5 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m137.5/137.5 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.0/51.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m76.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m65.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for matplotlib (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires ipykernel==6.17.1, but you have ipykernel 6.29.5 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.3 which is incompatible.\n",
            "google-adk 1.21.0 requires mcp<2.0.0,>=1.10.0, but you have mcp 1.9.0 which is incompatible.\n",
            "google-adk 1.21.0 requires requests<3.0.0,>=2.32.4, but you have requests 2.32.3 which is incompatible.\n",
            "google-adk 1.21.0 requires starlette<1.0.0,>=0.49.1, but you have starlette 0.46.2 which is incompatible.\n",
            "plotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.1 which is incompatible.\n",
            "inequality 1.1.2 requires matplotlib>=3.8, but you have matplotlib 3.7.1 which is incompatible.\n",
            "arviz 0.22.0 requires matplotlib>=3.8, but you have matplotlib 3.7.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mâœ… All imports successful including LLM, RUL, and Feedback Loop support\n"
          ]
        }
      ],
      "source": [
        " %pip install langchain_openai langchain langchain-community vinagent==0.0.6 numpy pandas scikit-learn tensorflow langgraph scipy matplotlib seaborn opendatasets spektral paho-mqtt shap -q\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import gc\n",
        "import os\n",
        "import warnings\n",
        "import threading\n",
        "import time\n",
        "import queue\n",
        "from datetime import datetime\n",
        "from collections import defaultdict, deque\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from typing import Dict, Any, List, Tuple\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, matthews_corrcoef, mean_absolute_error, mean_squared_error\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Dense, RepeatVector, TimeDistributed, Input, Dropout, MultiHeadAttention, LayerNormalization, Conv1D, Add, GlobalAveragePooling1D\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from langchain_community.llms import OpenAI  # âœ… correct\n",
        "\n",
        "from vinagent.multi_agent import AgentNode\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import langchain\n",
        "langchain.debug = False\n",
        "\n",
        "import paho.mqtt.client as mqtt\n",
        "from abc import ABC, abstractmethod\n",
        "\n",
        "import shap\n",
        "\n",
        "# NEW: Import for LLM integration\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "# NEW: Import for statistical testing (drift detection)\n",
        "from scipy.stats import ks_2samp\n",
        "\n",
        "print(\"âœ… All imports successful including LLM, RUL, and Feedback Loop support\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_wJ7OMICr1U",
        "outputId": "3bbd149c-c224-4925-d34d-85c3c9fe8d3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Message Broker initialized\n"
          ]
        }
      ],
      "source": [
        "MQTT_CONFIG = {\n",
        "    'broker': 'localhost',\n",
        "    'port': 1883,\n",
        "    'keepalive': 60,\n",
        "    'qos': 1,\n",
        "    'tls_version': None,\n",
        "    'use_tls': False,\n",
        "    'topics': {\n",
        "        'edge_stream_1': 'chunk/stream1',\n",
        "        'edge_stream_2': 'chunk/stream2',\n",
        "        'if_scores': 'scores/if',\n",
        "        'transformer_scores': 'scores/transformer',\n",
        "        'rul_predictions': 'predictions/rul',\n",
        "        'anomalies': 'anomalies',\n",
        "        'actions': 'actions',\n",
        "        'local_feedback': 'feedback/edge',\n",
        "        'operator_feedback': 'feedback/operator',\n",
        "        'feedback_metrics': 'feedback/metrics',\n",
        "        'policy_updates': 'policy/updates',\n",
        "        'knowledge_graph': 'graph/sync',\n",
        "        'monitoring_logs': 'monitor/logs',\n",
        "        'evaluation_metrics': 'metrics/evaluation'\n",
        "    }   \n",
        "}\n",
        "\n",
        "class MessageBroker:\n",
        "    def __init__(self):\n",
        "        self.topics = defaultdict(deque)\n",
        "        self.subscribers = defaultdict(list)\n",
        "        self.lock = threading.Lock()\n",
        "        self.message_history = defaultdict(list)\n",
        "\n",
        "    def publish(self, topic: str, payload: Dict[str, Any], qos: int = 1):\n",
        "        with self.lock:\n",
        "            message = {'timestamp': datetime.now().isoformat(), 'topic': topic, 'payload': payload, 'qos': qos}\n",
        "            self.topics[topic].append(message)\n",
        "            self.message_history[topic].append(message)\n",
        "            for callback in self.subscribers[topic]:\n",
        "                try:\n",
        "                    callback(topic, message)\n",
        "                except Exception as e:\n",
        "                    pass\n",
        "\n",
        "    def subscribe(self, topic: str, callback):\n",
        "        with self.lock:\n",
        "            self.subscribers[topic].append(callback)\n",
        "\n",
        "    def get_latest(self, topic: str, default=None):\n",
        "        with self.lock:\n",
        "            if self.topics[topic]:\n",
        "                return self.topics[topic][-1]\n",
        "            return default\n",
        "\n",
        "    def get_all(self, topic: str):\n",
        "        with self.lock:\n",
        "            return list(self.topics[topic])\n",
        "\n",
        "message_broker = MessageBroker()\n",
        "print(\"âœ… Message Broker initialized\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YaruRqZ3Cr1V",
        "outputId": "a973be30-7c98-4e5b-d3cd-7ddde7d8f4c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ğŸ“‚ Loading Boiler Dataset...\n",
            "âœ… Boiler loaded: (27280, 7)\n",
            "\n",
            "ğŸ“‚ Loading Wind Turbine Dataset...\n",
            "âœ… Wind Turbine dataset loaded from dataset/iiot-data-of-wind-turbine/\n",
            "âœ… Boiler - Train (21824, 6), Test (5456, 6)\n",
            "âœ… Wind Turbine - Train (263, 57), Test (66, 57)\n",
            "\n",
            "âœ… Data loading complete\n",
            "   Boiler anomaly class distribution: {0: 3696, 1: 1760}\n",
            "   Wind Turbine anomaly class distribution: {np.int64(1): 61, np.int64(0): 5}\n",
            "   Wind Turbine RUL threshold: 25 hours (RUL <= 25 = anomaly)\n",
            "   Data source: Real dataset\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "print(\"\\nğŸ“‚ Loading Boiler Dataset...\")\n",
        "data_boiler = pd.read_csv('dataset/Boiler_emulator_dataset.csv')\n",
        "print(f\"âœ… Boiler loaded: {data_boiler.shape}\")\n",
        "\n",
        "print(\"\\nğŸ“‚ Loading Wind Turbine Dataset...\")\n",
        "try:\n",
        "    scada_df = pd.read_csv('dataset/iiot-data-of-wind-turbine/scada_data.csv')\n",
        "    scada_df['DateTime'] = pd.to_datetime(scada_df['DateTime'])\n",
        "\n",
        "    fault_df = pd.read_csv('dataset/iiot-data-of-wind-turbine/fault_data.csv')\n",
        "    fault_df['DateTime'] = pd.to_datetime(fault_df['DateTime'])\n",
        "\n",
        "    status_df = pd.read_csv('dataset/iiot-data-of-wind-turbine/status_data.csv')\n",
        "    status_df['Time'] = pd.to_datetime(status_df['Time'])\n",
        "    status_df.rename(columns={'Time': 'DateTime'}, inplace=True)\n",
        "\n",
        "    print(\"âœ… Wind Turbine dataset loaded from dataset/iiot-data-of-wind-turbine/\")\n",
        "    data_source = \"Real dataset\"\n",
        "\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"âš ï¸  Dataset not found: {e}\")\n",
        "    print(\"ğŸ“ To fix this: Download the wind turbine CSV files and place them in:\")\n",
        "    print(\"   dataset/iiot-data-of-wind-turbine/\")\n",
        "    print(\"   â”œâ”€â”€ scada_data.csv\")\n",
        "    print(\"   â”œâ”€â”€ fault_data.csv\")\n",
        "    print(\"   â””â”€â”€ status_data.csv\")\n",
        "    print(\"\\nğŸ’¾ Generating synthetic wind turbine data as fallback...\\n\")\n",
        "\n",
        "    n_samples = 1000\n",
        "    n_features = 40\n",
        "    np.random.seed(42)\n",
        "    scada_data = {'DateTime': pd.date_range('2014-04-01', periods=n_samples, freq='10min')}\n",
        "    for i in range(n_features):\n",
        "        scada_data[f'Sensor_{i}'] = np.random.normal(50, 15, n_samples)\n",
        "\n",
        "    scada_df = pd.DataFrame(scada_data)\n",
        "    scada_df['DateTime'] = pd.to_datetime(scada_df['DateTime'])\n",
        "\n",
        "    fault_types = ['gf', 'mf', 'ff', 'af', 'ef']\n",
        "    fault_data = {\n",
        "        'DateTime': scada_df['DateTime'].sample(n=200, random_state=42).sort_values().reset_index(drop=True),\n",
        "        'Fault': np.random.choice(fault_types, size=200)\n",
        "    }\n",
        "    fault_df = pd.DataFrame(fault_data)\n",
        "\n",
        "    status_df = pd.DataFrame({\n",
        "        'DateTime': scada_df['DateTime'],\n",
        "        'Status': np.random.choice(['OK', 'WARNING', 'ERROR'], size=n_samples, p=[0.7, 0.2, 0.1])\n",
        "    })\n",
        "\n",
        "    print(\"âœ… Synthetic wind turbine data generated\")\n",
        "    data_source = \"Synthetic (fallback)\"\n",
        "\n",
        "df_turbine = scada_df.merge(fault_df[['DateTime', 'Fault']], on='DateTime', how='left')\n",
        "df_turbine['Fault'] = df_turbine['Fault'].replace(np.nan, 'NF')\n",
        "\n",
        "df_nf = df_turbine[df_turbine['Fault']=='NF']\n",
        "if len(df_nf) > 300:\n",
        "    df_nf = df_nf.sample(n=300, random_state=42)\n",
        "\n",
        "df_f = df_turbine[df_turbine['Fault']!='NF']\n",
        "df_turbine = pd.concat((df_nf, df_f), axis=0).reset_index(drop=True)\n",
        "\n",
        "irrelevant_cols = ['DateTime', 'WEC: ava. windspeed', 'WEC: ava. available P from wind',\n",
        "                   'WEC: ava. available P technical reasons', 'WEC: ava. Available P force majeure reasons',\n",
        "                   'WEC: ava. Available P force external reasons', 'WEC: max. windspeed', 'WEC: min. windspeed',\n",
        "                   'WEC: Operating Hours', 'WEC: Production kWh', 'WEC: Production minutes']\n",
        "irrelevant_cols = [col for col in irrelevant_cols if col in df_turbine.columns]\n",
        "df_turbine_clean = df_turbine.drop(columns=irrelevant_cols, errors='ignore')\n",
        "\n",
        "X_turbine_full = df_turbine_clean.drop('Fault', axis=1)\n",
        "X_turbine_full['sensor_diff_0_1'] = X_turbine_full.iloc[:, 0] - X_turbine_full.iloc[:, 1]\n",
        "X_turbine_full['sensor_change_0'] = X_turbine_full.iloc[:, 0].diff().fillna(0)\n",
        "y_turbine_full = df_turbine_clean['Fault']\n",
        "\n",
        "le_turbine = LabelEncoder()\n",
        "y_turbine_encoded = le_turbine.fit_transform(y_turbine_full)\n",
        "\n",
        "X_train_turbine, X_test_turbine, y_train_turbine, y_test_turbine = train_test_split(\n",
        "    X_turbine_full, y_turbine_encoded, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "scaler_wt = StandardScaler()\n",
        "X_train_turbine_scaled = pd.DataFrame(scaler_wt.fit_transform(X_train_turbine), columns=X_train_turbine.columns)\n",
        "X_test_turbine_scaled = pd.DataFrame(scaler_wt.transform(X_test_turbine), columns=X_test_turbine.columns)\n",
        "\n",
        "df_clean = data_boiler.copy()\n",
        "le_condition = LabelEncoder()\n",
        "df_clean['Condition'] = le_condition.fit_transform(df_clean['Condition'])\n",
        "le_class = LabelEncoder()\n",
        "df_clean['Class'] = le_class.fit_transform(df_clean['Class'])\n",
        "numeric_cols = ['Fuel_Mdot', 'Tair', 'Treturn', 'Tsupply', 'Water_Mdot']\n",
        "for col in numeric_cols:\n",
        "    df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')\n",
        "df_clean = df_clean.dropna()\n",
        "X_boiler = df_clean.drop('Class', axis=1)\n",
        "y_boiler = df_clean['Class']\n",
        "\n",
        "# Add domain-specific features BEFORE splitting\n",
        "#X_boiler['temp_diff'] = X_boiler['Tsupply'] - X_boiler['Treturn']\n",
        "#X_boiler['temp_change'] = X_boiler['Tsupply'].diff().fillna(0)\n",
        "\n",
        "# Add rolling statistics (3-point windows)\n",
        "#for col in ['Tsupply', 'Treturn', 'FuelMdot']:\n",
        " #   X_boiler[f'{col}_rolling_mean'] = X_boiler[col].rolling(window=3, min_periods=1).mean()\n",
        "#    X_boiler[f'{col}_rolling_std'] = X_boiler[col].rolling(window=3, min_periods=1).std().fillna(0)\n",
        "\n",
        "\n",
        "X_train_boiler_full, X_test_boiler, y_train_boiler_full, y_test_boiler = train_test_split(\n",
        "    X_boiler, y_boiler, test_size=0.2, random_state=42, stratify=y_boiler\n",
        ")\n",
        "\n",
        "scaler_b = StandardScaler()\n",
        "X_train_boiler = pd.DataFrame(scaler_b.fit_transform(X_train_boiler_full), columns=X_train_boiler_full.columns)\n",
        "X_test_boiler = pd.DataFrame(scaler_b.transform(X_test_boiler), columns=X_test_boiler.columns)\n",
        "\n",
        "\n",
        "print(f\"âœ… Boiler - Train {X_train_boiler.shape}, Test {X_test_boiler.shape}\")\n",
        "print(f\"âœ… Wind Turbine - Train {X_train_turbine_scaled.shape}, Test {X_test_turbine_scaled.shape}\")\n",
        "\n",
        "y_test_boiler_anomaly = (y_test_boiler == 1).astype(int)\n",
        "y_test_turbine_anomaly = (y_test_turbine > 0).astype(int)\n",
        "\n",
        "y_test_boiler_rul = pd.Series(\n",
        "    np.where(y_test_boiler == 1, np.random.randint(5, 30, len(y_test_boiler)), np.random.randint(30, 100, len(y_test_boiler)))\n",
        ")\n",
        "y_test_turbine_rul = pd.Series(\n",
        "    np.where(y_test_turbine_anomaly == 1, np.random.randint(5, 25, len(y_test_turbine)), np.random.randint(25, 100, len(y_test_turbine)))\n",
        ")\n",
        "\n",
        "print(f\"\\nâœ… Data loading complete\")\n",
        "print(f\"   Boiler anomaly class distribution: {y_test_boiler_anomaly.value_counts().to_dict()}\")\n",
        "print(f\"   Wind Turbine anomaly class distribution: {dict(Counter(y_test_turbine_anomaly))}\")\n",
        "print(f\"   Wind Turbine RUL threshold: 25 hours (RUL <= 25 = anomaly)\")\n",
        "print(f\"   Data source: {data_source}\")\n",
        "\n",
        "Xtr_boiler, Xte_boiler = X_train_boiler, X_test_boiler\n",
        "yte_boiler_anom, yte_boiler_rul = y_test_boiler_anomaly.values, y_test_boiler_rul.values\n",
        "Xtr_turbine, Xte_turbine = X_train_turbine_scaled, X_test_turbine_scaled\n",
        "yte_turbine_anom, yte_turbine_rul = y_test_turbine_anomaly, y_test_turbine_rul.values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSJdrhV-Cr1W",
        "outputId": "f6a8b220-8387-43a6-beb2-1f3a45a24d99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Configuration & Base Classes initialized\n"
          ]
        }
      ],
      "source": [
        "CONFIG = {\n",
        "    'datasets': {\n",
        "        'boiler': {\n",
        "            'X_train': X_train_boiler,\n",
        "            'X_test': X_test_boiler,\n",
        "            'y_train': y_train_boiler_full,\n",
        "            'y_test': y_test_boiler,\n",
        "            'y_test_anomaly': y_test_boiler_anomaly,\n",
        "            'y_test_rul': y_test_boiler_rul,\n",
        "            'scaler': scaler_b,\n",
        "            'name': 'Boiler Emulator',\n",
        "        },\n",
        "        'wind_turbine': {\n",
        "            'X_train': X_train_turbine_scaled,\n",
        "            'X_test': X_test_turbine_scaled,\n",
        "            'y_train': y_train_turbine,\n",
        "            'y_test': y_test_turbine,\n",
        "            'y_test_anomaly': y_test_turbine_anomaly,\n",
        "            'y_test_rul': y_test_turbine_rul,\n",
        "            'scaler': scaler_wt,\n",
        "            'name': 'Wind Turbine',\n",
        "        },\n",
        "    },\n",
        "    'mqtt_broker': message_broker,\n",
        "}\n",
        "\n",
        "class MQTTAgent(ABC):\n",
        "    def __init__(self, name: str, broker: 'MessageBroker', subscribed_topics: List[str] = None):\n",
        "        self.name = name\n",
        "        self.broker = broker\n",
        "        self.subscribed_topics = subscribed_topics or []\n",
        "        self.message_queue = queue.Queue()\n",
        "        self.execution_log = []\n",
        "        for topic in self.subscribed_topics:\n",
        "            self.broker.subscribe(topic, self.message_handler)\n",
        "\n",
        "    def message_handler(self, topic: str, message: Dict[str, Any]):\n",
        "        self.message_queue.put({'topic': topic, 'message': message})\n",
        "\n",
        "    def publish(self, topic: str, payload: Dict[str, Any], qos: int = None):\n",
        "        qos = qos or MQTT_CONFIG['qos']\n",
        "        self.broker.publish(topic, payload, qos=qos)\n",
        "        self.execution_log.append({'timestamp': datetime.now().isoformat(), 'action': 'publish', 'topic': topic, 'agent': self.name})\n",
        "\n",
        "    def get_latest_message(self, topic: str):\n",
        "        return self.broker.get_latest(topic)\n",
        "\n",
        "    @abstractmethod\n",
        "    def execute(self, *args, **kwargs):\n",
        "        pass\n",
        "\n",
        "print(\"âœ… Configuration & Base Classes initialized\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_HsL8U_CCr1W",
        "outputId": "da419937-4fbd-42d3-af4a-1eec84c3ed8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Edge Layer Agents (A1, A2) defined\n"
          ]
        }
      ],
      "source": [
        "class EdgeSubagentA1(MQTTAgent):\n",
        "    def __init__(self, name='A1_Temperature', broker=None, window_size=10):\n",
        "        super().__init__(name, broker or message_broker)\n",
        "        self.window_size = window_size\n",
        "        self.publishing_topic = MQTT_CONFIG['topics']['edge_stream_1']\n",
        "\n",
        "    def compute_features(self, data: np.ndarray) -> Dict[str, np.ndarray]:\n",
        "        rms = np.sqrt(np.mean(data**2, axis=0))\n",
        "        kurtosis = np.mean((data - np.mean(data, axis=0))**4, axis=0) / (np.std(data, axis=0)**4 + 1e-8)\n",
        "        skewness = np.mean((data - np.mean(data, axis=0))**3, axis=0) / (np.std(data, axis=0)**3 + 1e-8)\n",
        "        x = np.arange(len(data))\n",
        "        trend = np.array([np.polyfit(x, data[:, i], 1)[0] for i in range(data.shape[1])])\n",
        "        fft_vals = np.fft.fft(data, axis=0)\n",
        "        spectral_energy = np.abs(fft_vals).mean(axis=0)\n",
        "        stats = {'mean': np.mean(data, axis=0), 'std': np.std(data, axis=0)}\n",
        "        return {'rms': rms, 'kurtosis': kurtosis, 'skewness': skewness, 'trend': trend, 'spectral_energy': spectral_energy, 'stats': stats}\n",
        "\n",
        "    def execute(self, data: np.ndarray, timestamp: float = None):\n",
        "        timestamp = timestamp or datetime.now().timestamp()\n",
        "        features = self.compute_features(data)\n",
        "        payload = {\n",
        "            'timestamp': timestamp,\n",
        "            'agent': self.name,\n",
        "            'window_size': self.window_size,\n",
        "            'data_shape': data.shape,\n",
        "            'features': {'rms': features['rms'].tolist(), 'kurtosis': features['kurtosis'].tolist()},\n",
        "        }\n",
        "        self.publish(self.publishing_topic, payload)\n",
        "        return features\n",
        "\n",
        "class EdgeSubagentA2(EdgeSubagentA1):\n",
        "    def __init__(self, name='A2_Vibration', broker=None, window_size=10):\n",
        "        super().__init__(name, broker, window_size)\n",
        "        self.publishing_topic = MQTT_CONFIG['topics']['edge_stream_2']\n",
        "\n",
        "print(\"âœ… Edge Layer Agents (A1, A2) defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19UUbA-ICr1X",
        "outputId": "a5351e26-56e2-4991-cd13-cd39a4e981da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Fog Orchestration Agent defined\n"
          ]
        }
      ],
      "source": [
        "class FogOrchestrationAgentB(MQTTAgent):\n",
        "    def __init__(self, name='B_Orchestrator', broker=None):\n",
        "        subscribed_topics = [MQTT_CONFIG['topics']['edge_stream_1'], MQTT_CONFIG['topics']['edge_stream_2']]\n",
        "        super().__init__(name, broker or message_broker, subscribed_topics)\n",
        "        self.score_history = deque(maxlen=100)\n",
        "\n",
        "    def detect_drift(self, anomaly_scores: np.ndarray, window_size: int = 50) -> bool:\n",
        "        if len(anomaly_scores) > 0:\n",
        "            self.score_history.extend(anomaly_scores)\n",
        "        if len(self.score_history) < window_size:\n",
        "            return False\n",
        "        recent = list(self.score_history)[-window_size:]\n",
        "        historical = list(self.score_history)[:-window_size]\n",
        "        if len(historical) < 10:\n",
        "            return False\n",
        "        statistic, p_value = ks_2samp(historical, recent)\n",
        "        if p_value < 0.05:\n",
        "            self.publish(MQTT_CONFIG['topics']['local_feedback'], {'drift_detected': True, 'p_value': float(p_value)})\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def execute(self, X: np.ndarray = None) -> Dict[str, Any]:\n",
        "        msg_a1 = self.get_latest_message(MQTT_CONFIG['topics']['edge_stream_1'])\n",
        "        msg_a2 = self.get_latest_message(MQTT_CONFIG['topics']['edge_stream_2'])\n",
        "        if msg_a1 and msg_a2:\n",
        "            payload = {\n",
        "                'timestamp': datetime.now().isoformat(),\n",
        "                'agent': self.name,\n",
        "                'a1_status': 'received',\n",
        "                'a2_status': 'received'\n",
        "            }\n",
        "            self.publish(MQTT_CONFIG['topics']['monitoring_logs'], payload)\n",
        "            return {'status': 'orchestration_complete', 'agents_active': 2}\n",
        "        return {'status': 'waiting_for_data', 'agents_active': 0}\n",
        "\n",
        "print(\"âœ… Fog Orchestration Agent defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWouu2-YCr1X",
        "outputId": "c7035a59-3b7d-40b0-ca18-d85d7526304f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Fog Detection Agents B1, B2 defined\n"
          ]
        }
      ],
      "source": [
        "class FogSubagentB1(MQTTAgent):\n",
        "    def __init__(self, name='B1_IsolationForest', broker=None, contamination=0.32):\n",
        "        super().__init__(name, broker or message_broker)\n",
        "        self.contamination = contamination\n",
        "        self.model = IsolationForest(contamination=self.contamination, random_state=42, n_estimators=200, max_samples=256)\n",
        "        self.is_trained = False\n",
        "        self.X_train = None\n",
        "\n",
        "    def train(self, X_train):\n",
        "        self.X_train = X_train  # Store training data for retraining\n",
        "        self.model.fit(X_train)\n",
        "        self.is_trained = True\n",
        "        print(f'   âœ… B1 trained with contamination={self.contamination:.4f}')\n",
        "\n",
        "    def update_contamination(self, new_contamination: float):\n",
        "        \"\"\"Update contamination parameter and retrain model\"\"\"\n",
        "        if abs(new_contamination - self.contamination) > 0.01:  # Only retrain if significant change\n",
        "            print(f'   ğŸ”„ B1 Retraining: contamination {self.contamination:.4f} â†’ {new_contamination:.4f}')\n",
        "            self.contamination = new_contamination\n",
        "            self.model = IsolationForest(contamination=self.contamination, random_state=42, n_estimators=200, max_samples=256)\n",
        "            if self.X_train is not None:\n",
        "                self.model.fit(self.X_train)\n",
        "                self.is_trained = True\n",
        "\n",
        "    def execute(self, X: np.ndarray) -> Dict[str, Any]:\n",
        "        if not self.is_trained:\n",
        "            return None\n",
        "        predictions = self.model.predict(X)\n",
        "        scores = self.model.score_samples(X)\n",
        "        payload = {'timestamp': datetime.now().isoformat(), 'agent': self.name, 'predictions': predictions.tolist(), 'anomaly_scores': scores.tolist()}\n",
        "        self.publish(MQTT_CONFIG['topics']['if_scores'], payload)\n",
        "        return {'predictions': predictions, 'scores': scores}\n",
        "\n",
        "class FogSubagentB2(MQTTAgent):\n",
        "    def __init__(self, name='B2_TimeSeriesTransformer', broker=None):\n",
        "        super().__init__(name, broker or message_broker)\n",
        "        self.model = None\n",
        "        self.is_trained = False\n",
        "\n",
        "    def build_transformer_model(self, input_dim: int, seq_length: int = 10, d_model: int = 128):\n",
        "        inputs = Input(shape=(seq_length, input_dim))\n",
        "        x = Conv1D(d_model, kernel_size=1, padding='same', activation='relu')(inputs)\n",
        "        attention = MultiHeadAttention(num_heads=4, key_dim=16)\n",
        "        attn_output = attention(x, x)\n",
        "        x = Add()([x, attn_output])\n",
        "        x = LayerNormalization()(x)\n",
        "        x = Dropout(0.2)(x)\n",
        "        x = GlobalAveragePooling1D()(x)\n",
        "        x = Dense(32, activation='relu')(x)\n",
        "        outputs = Dense(1, activation='sigmoid')(x)\n",
        "        model = Model(inputs=inputs, outputs=outputs)\n",
        "        model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy')\n",
        "        self.model = model\n",
        "\n",
        "    def train(self, X_train):\n",
        "        input_dim = X_train.shape[1]\n",
        "        self.build_transformer_model(input_dim)\n",
        "        X_seq = self.create_sequences(X_train, seq_length=10)\n",
        "        y_train = np.zeros(len(X_seq))\n",
        "        self.model.fit(X_seq, y_train, epochs=25, batch_size=16, verbose=0, callbacks=[EarlyStopping(monitor='loss', patience=2)])\n",
        "        self.is_trained = True\n",
        "\n",
        "    def execute(self, X: np.ndarray) -> Dict[str, Any]:\n",
        "        if not self.is_trained:\n",
        "            return None\n",
        "        X_seq = self.create_sequences(X, seq_length=10)\n",
        "        scores = self.model.predict(X_seq, verbose=0).flatten()\n",
        "        scores = np.pad(scores, (9, len(X) - len(scores) - 9), mode='edge')\n",
        "        payload = {'timestamp': datetime.now().isoformat(), 'agent': self.name, 'anomaly_scores': scores.tolist()}\n",
        "        self.publish(MQTT_CONFIG['topics']['transformer_scores'], payload)\n",
        "        return {'scores': scores}\n",
        "\n",
        "    def create_sequences(self, data, seq_length=10):\n",
        "        sequences = []\n",
        "        for i in range(len(data) - seq_length + 1):\n",
        "            sequences.append(data[i:i+seq_length])\n",
        "        return np.array(sequences) if sequences else np.array([])\n",
        "\n",
        "print(\"âœ… Fog Detection Agents B1, B2 defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iluxIduOCr1X",
        "outputId": "dbe41916-f7be-4a51-dc01-733372487a2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… RUL Prediction Agent defined\n"
          ]
        }
      ],
      "source": [
        "class FogSubagentB2_RUL(MQTTAgent):\n",
        "    def __init__(self, name='B2_RUL_Predictor', broker=None):\n",
        "        super().__init__(name, broker or message_broker)\n",
        "        self.model = None\n",
        "        self.is_trained = False\n",
        "\n",
        "    def build_rul_model(self, input_dim: int):\n",
        "        from tensorflow.keras.layers import LSTM\n",
        "        inputs = Input(shape=(10, input_dim))\n",
        "        x = LSTM(64, return_sequences=True)(inputs)\n",
        "        x = LSTM(32)(x)\n",
        "        x = Dense(32, activation='relu')(x)\n",
        "        outputs = Dense(1, activation='relu')(x)\n",
        "        model = Model(inputs=inputs, outputs=outputs)\n",
        "        model.compile(optimizer=Adam(learning_rate=0.001), loss='mae')\n",
        "        self.model = model\n",
        "\n",
        "    def train(self, X_train, RUL_train):\n",
        "        input_dim = X_train.shape[1]\n",
        "        self.build_rul_model(input_dim)\n",
        "        X_seq = self.create_sequences(X_train, seq_length=10)\n",
        "        if len(X_seq) == 0:\n",
        "            self.is_trained = True\n",
        "            return\n",
        "        RUL_train_array = RUL_train.values if hasattr(RUL_train, 'values') else RUL_train\n",
        "        if len(RUL_train_array) < len(X_train):\n",
        "            RUL_train_array = np.pad(RUL_train_array, (0, len(X_train) - len(RUL_train_array)), mode='edge')\n",
        "        RUL_seq = RUL_train_array[9:9+len(X_seq)]\n",
        "        if len(RUL_seq) != len(X_seq):\n",
        "            if len(RUL_seq) < len(X_seq):\n",
        "                RUL_seq = np.pad(RUL_seq, (0, len(X_seq) - len(RUL_seq)), mode='edge')\n",
        "            else:\n",
        "                RUL_seq = RUL_seq[:len(X_seq)]\n",
        "        self.model.fit(X_seq, RUL_seq, epochs=10, batch_size=32, verbose=0, callbacks=[EarlyStopping(monitor='loss', patience=2)])\n",
        "        self.is_trained = True\n",
        "\n",
        "    def execute(self, X: np.ndarray) -> Dict[str, Any]:\n",
        "        if not self.is_trained:\n",
        "            return None\n",
        "        X_seq = self.create_sequences(X, seq_length=10)\n",
        "        if len(X_seq) == 0:\n",
        "            return None\n",
        "        rul_predictions = self.model.predict(X_seq, verbose=0).flatten()\n",
        "        rul_predictions = np.pad(rul_predictions, (9, len(X) - len(rul_predictions) - 9), mode='edge')\n",
        "        payload = {'timestamp': datetime.now().isoformat(), 'agent': self.name, 'rul_predictions': rul_predictions.tolist()}\n",
        "        self.publish(MQTT_CONFIG['topics']['rul_predictions'], payload)\n",
        "        return {'rul_predictions': rul_predictions}\n",
        "\n",
        "    def create_sequences(self, data, seq_length=10):\n",
        "        sequences = []\n",
        "        for i in range(len(data) - seq_length + 1):\n",
        "            sequences.append(data[i:i+seq_length])\n",
        "        return np.array(sequences) if sequences else np.array([])\n",
        "\n",
        "print(\"âœ… RUL Prediction Agent defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_XjSPmjCr1Y",
        "outputId": "68affafc-e2ff-4381-a918-8c7293f8bee4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Fog Detection Agents B3, C defined\n"
          ]
        }
      ],
      "source": [
        "class FogSubagentB3(MQTTAgent):\n",
        "    def __init__(self, name='B3_ConsensusVoting', broker=None):\n",
        "        super().__init__(name, broker or message_broker)\n",
        "\n",
        "    def aggregate(self) -> Dict[str, Any]:\n",
        "        msg_b1 = self.get_latest_message(MQTT_CONFIG['topics']['if_scores'])\n",
        "        msg_b2 = self.get_latest_message(MQTT_CONFIG['topics']['transformer_scores'])\n",
        "        weights = self.get_latest_message(MQTT_CONFIG['topics']['knowledge_graph'])\n",
        "        print(f'weights {weights}')\n",
        "        if weights:\n",
        "            w1 = weights.get('payload').get('w1', 0.4)\n",
        "        else:\n",
        "            w1 = 0.4\n",
        "        w2 = 1-w1\n",
        "        if msg_b1 and msg_b2:\n",
        "            scores_b1 = np.array(msg_b1['payload']['anomaly_scores'])\n",
        "            scores_b2 = np.array(msg_b2['payload']['anomaly_scores'])\n",
        "            scores_b1_norm = (scores_b1 - scores_b1.min()) / (scores_b1.max() - scores_b1.min() + 1e-8)\n",
        "            scores_b2_norm = (scores_b2 - scores_b2.min()) / (scores_b2.max() - scores_b2.min() + 1e-8)\n",
        "            msg_b1 = self.get_latest_message(MQTT_CONFIG['topics']['if_scores'])\n",
        "            msg_b1 = self.get_latest_message(MQTT_CONFIG['topics']['if_scores'])\n",
        "            consensus_scores = w1 * scores_b1_norm + w2 * scores_b2_norm\n",
        "            print(f\"w1: {w1}; w2: {w2}\")\n",
        "            print(f'top 5 consensus_scores: {consensus_scores[:5]}')\n",
        "            # FIX: Return normalized scores, not raw scores\n",
        "            return {'consensus_scores': consensus_scores, 'scores_b1_norm': scores_b1_norm, 'scores_b2_norm': scores_b2_norm}\n",
        "        return None\n",
        "\n",
        "    def execute(self) -> Dict[str, Any]:\n",
        "        result = self.aggregate()\n",
        "        if result:\n",
        "            payload = {\n",
        "                'timestamp': datetime.now().isoformat(),\n",
        "                'agent': self.name,\n",
        "                'consensus_scores': result['consensus_scores'].tolist(),\n",
        "                'scores_b1_norm': result['scores_b1_norm'].tolist(),\n",
        "                'scores_b2_norm': result['scores_b2_norm'].tolist()\n",
        "            }\n",
        "            self.publish(MQTT_CONFIG['topics']['anomalies'], payload)\n",
        "            return result\n",
        "        return None\n",
        "\n",
        "class FogSubagentC(MQTTAgent):\n",
        "    def __init__(self, name='C_ResponseGenerator', broker=None, use_mock=True):\n",
        "        super().__init__(name, broker or message_broker)\n",
        "        self.use_mock = use_mock\n",
        "\n",
        "    def generate_explanation_llm(self, anomaly_scores: np.ndarray) -> Dict[str, Any]:\n",
        "        severity = float(np.max(anomaly_scores))\n",
        "        if severity > 0.7:\n",
        "            return {'explanation': 'Critical bearing wear detected', 'action': 'IMMEDIATE_SHUTDOWN', 'priority': 'HIGH', 'downtime': 8}\n",
        "        elif severity > 0.4:\n",
        "            return {'explanation': 'Moderate anomaly', 'action': 'SCHEDULE_INSPECTION', 'priority': 'MEDIUM', 'downtime': 4}\n",
        "        else:\n",
        "            return {'explanation': 'Minor drift', 'action': 'CONTINUE_MONITORING', 'priority': 'LOW', 'downtime': 0}\n",
        "\n",
        "    def execute(self) -> Dict[str, Any]:\n",
        "        anomaly_msg = self.get_latest_message(MQTT_CONFIG['topics']['anomalies'])\n",
        "        if anomaly_msg:\n",
        "            scores = np.array(anomaly_msg['payload']['consensus_scores'])\n",
        "            explanation_dict = self.generate_explanation_llm(scores)\n",
        "            payload = {'timestamp': datetime.now().isoformat(), 'agent': self.name, **explanation_dict}\n",
        "            self.publish(MQTT_CONFIG['topics']['actions'], payload)\n",
        "            return explanation_dict\n",
        "        return None\n",
        "\n",
        "print(\"âœ… Fog Detection Agents B3, C defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_maWza55Cr1Y",
        "outputId": "647d7ee8-4a05-478e-ea73-2097ff9b621b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Cloud Agents D, E defined\n"
          ]
        }
      ],
      "source": [
        "class CloudAgentD(MQTTAgent):\n",
        "    def __init__(self, name='D_EvolutionAgent_PPO', broker=None):\n",
        "        super().__init__(name, broker or message_broker)\n",
        "        self.policy_params = {\n",
        "            'contamination_rate': 0.32,\n",
        "            'threshold_delta_consensus': 0.0,  # Store DELTAS not absolute values\n",
        "            'threshold_delta_b1': 0.0,\n",
        "            'threshold_delta_b2': 0.0,\n",
        "            'w1': 0.4,\n",
        "            'w2': 0.6\n",
        "        }\n",
        "        self.data_quantity = 0\n",
        "        self.learning_rates = {\n",
        "            'w1': 0.1,\n",
        "            'contamination': 0.02,\n",
        "            'threshold': 0.05\n",
        "        }\n",
        "\n",
        "    def update_policy_ppo(self, metrics: Dict) -> Dict:\n",
        "        f1_consensus = metrics.get('consensus').get('f1', 0.5)\n",
        "        f1_b1_score = metrics.get('scores_b1_norm').get('f1', 0.2)\n",
        "        f1_b2_score = metrics.get('scores_b2_norm').get('f1', 0.2)\n",
        "\n",
        "        # Update w1/w2 weights\n",
        "        delta_w1 = self.learning_rates['w1'] * (f1_b1_score - f1_b2_score)\n",
        "\n",
        "        weights = self.get_latest_message(MQTT_CONFIG['topics']['knowledge_graph'])\n",
        "        if weights:\n",
        "            w1_current = weights.get('payload').get('w1', 0.4)\n",
        "            contamination_current = weights.get('payload').get('contamination_rate', 0.32)\n",
        "        else:\n",
        "            w1_current = self.policy_params['w1']\n",
        "            contamination_current = self.policy_params['contamination_rate']\n",
        "\n",
        "        epsilon = 0.005\n",
        "        w1_new = np.clip(w1_current + delta_w1, 0.2, 0.6) + epsilon\n",
        "        w2_new = 1.0 - w1_new\n",
        "\n",
        "        # Tune contamination based on F1 performance (increase if F1 < 0.6, decrease if F1 > 0.7)\n",
        "        if f1_consensus < 0.6:\n",
        "            delta_contamination = self.learning_rates['contamination']\n",
        "        elif f1_consensus > 0.7:\n",
        "            delta_contamination = -self.learning_rates['contamination']\n",
        "        else:\n",
        "            delta_contamination = 0.0\n",
        "        contamination_new = np.clip(contamination_current + delta_contamination, 0.05, 0.5)\n",
        "\n",
        "        # Adjust thresholds based on precision-recall balance\n",
        "        # Return DELTA values that will be added to current thresholds\n",
        "        precision_consensus = metrics.get('consensus').get('precision', 0.5)\n",
        "        recall_consensus = metrics.get('consensus').get('recall', 0.5)\n",
        "\n",
        "        # If precision > recall, lower threshold to increase recall (negative delta)\n",
        "        # If recall > precision, raise threshold to increase precision (positive delta)\n",
        "        if precision_consensus > recall_consensus + 0.1:\n",
        "            threshold_delta = -self.learning_rates['threshold']\n",
        "        elif recall_consensus > precision_consensus + 0.1:\n",
        "            threshold_delta = self.learning_rates['threshold']\n",
        "        else:\n",
        "            threshold_delta = 0.0\n",
        "\n",
        "        print(f'   ğŸ§  Policy Update: f1_consensus={f1_consensus:.4f}, f1_b1={f1_b1_score:.4f}, f1_b2={f1_b2_score:.4f}')\n",
        "        print(f'   ğŸ§  Weights: w1: {w1_current:.4f} â†’ {w1_new:.4f}')\n",
        "        print(f'   ğŸ§  Contamination: {contamination_current:.4f} â†’ {contamination_new:.4f} (Î”={delta_contamination:.4f})')\n",
        "        print(f'   ğŸ§  Threshold deltas: consensus: {threshold_delta:.4f}, b1: {threshold_delta*0.5:.4f}, b2: {threshold_delta*0.5:.4f}')\n",
        "\n",
        "        self.policy_params['w1'] = w1_new\n",
        "        self.policy_params['w2'] = w2_new\n",
        "        self.policy_params['contamination_rate'] = contamination_new\n",
        "        self.policy_params['threshold_delta_consensus'] = threshold_delta\n",
        "        self.policy_params['threshold_delta_b1'] = threshold_delta * 0.5\n",
        "        self.policy_params['threshold_delta_b2'] = threshold_delta * 0.5\n",
        "        self.data_quantity += 1\n",
        "\n",
        "        return {\n",
        "            'status': 'updated',\n",
        "            'w1': self.policy_params['w1'],\n",
        "            'w2': self.policy_params['w2'],\n",
        "            'contamination_rate': self.policy_params['contamination_rate'],\n",
        "            'threshold_delta_consensus': self.policy_params['threshold_delta_consensus'],\n",
        "            'threshold_delta_b1': self.policy_params['threshold_delta_b1'],\n",
        "            'threshold_delta_b2': self.policy_params['threshold_delta_b2']\n",
        "        }\n",
        "\n",
        "    def execute(self, metrics: Dict) -> Dict[str, Any]:\n",
        "        policy_update = self.update_policy_ppo(metrics)\n",
        "        knowledge_graph = {\n",
        "            'w1': float(self.policy_params['w1']),\n",
        "            'w2': float(self.policy_params['w2']),\n",
        "            'contamination_rate': float(self.policy_params['contamination_rate']),\n",
        "            'threshold_delta_consensus': float(self.policy_params['threshold_delta_consensus']),\n",
        "            'threshold_delta_b1': float(self.policy_params['threshold_delta_b1']),\n",
        "            'threshold_delta_b2': float(self.policy_params['threshold_delta_b2']),\n",
        "            'data_quantity': self.data_quantity\n",
        "        }\n",
        "        payload = {'timestamp': datetime.now().isoformat(), 'agent': self.name, 'policy': self.policy_params.copy(), 'knowledge_graph': knowledge_graph}\n",
        "        self.publish(MQTT_CONFIG['topics']['policy_updates'], payload)\n",
        "        self.publish(MQTT_CONFIG['topics']['knowledge_graph'], knowledge_graph)\n",
        "        return payload\n",
        "\n",
        "class CloudAgentE(MQTTAgent):\n",
        "    def __init__(self, name='E_MetaAgent_SHAP', broker=None, background_data: np.ndarray = None):\n",
        "        super().__init__(name, broker or message_broker)\n",
        "        self.audit_log = []\n",
        "\n",
        "    def execute(self, predictions: np.ndarray, scores: np.ndarray, y_true: np.ndarray, iteration: int) -> Dict[str, Any]:\n",
        "        pred_binary = (predictions > 0).astype(int)\n",
        "        metrics = {\n",
        "            'accuracy': float(accuracy_score(y_true, pred_binary)),\n",
        "            'precision': float(precision_score(y_true, pred_binary, zero_division=0)),\n",
        "            'recall': float(recall_score(y_true, pred_binary, zero_division=0)),\n",
        "            'f1': float(f1_score(y_true, pred_binary, zero_division=0)),\n",
        "            'roc_auc': float(roc_auc_score(y_true, scores) if len(np.unique(y_true)) > 1 else 0.5),\n",
        "            'mcc': float(matthews_corrcoef(y_true, pred_binary))\n",
        "        }\n",
        "        audit_report = {'iteration': iteration, 'metrics': metrics, 'status': 'PASS' if metrics['f1'] > 0.6 else 'REVIEW_NEEDED'}\n",
        "        self.audit_log.append(audit_report)\n",
        "        payload = {'timestamp': datetime.now().isoformat(), 'agent': self.name, 'iteration': iteration, 'audit_metrics': metrics}\n",
        "        self.publish(MQTT_CONFIG['topics']['monitoring_logs'], payload)\n",
        "        return payload\n",
        "\n",
        "print(\"âœ… Cloud Agents D, E defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHD65fHPCr1Z",
        "outputId": "05116094-3335-43b7-a796-017e73bda9b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… SEMAS COMPLETE System defined\n"
          ]
        }
      ],
      "source": [
        "class SEMAS_COMPLETE(MQTTAgent):\n",
        "    def __init__(self, name='SEMAS_COMPLETE', broker: MessageBroker = None, background_data: np.ndarray = None):\n",
        "        super().__init__(name, broker or message_broker)\n",
        "        self.broker = broker or message_broker\n",
        "        self.name = name\n",
        "        self.edge_a1 = EdgeSubagentA1(broker=self.broker)\n",
        "        self.edge_a2 = EdgeSubagentA2(broker=self.broker)\n",
        "        self.fog_b = FogOrchestrationAgentB(broker=self.broker)\n",
        "        self.fog_b1 = FogSubagentB1(broker=self.broker)\n",
        "        self.fog_b2 = FogSubagentB2(broker=self.broker)\n",
        "        self.fog_b2_rul = FogSubagentB2_RUL(broker=self.broker)\n",
        "        self.fog_b3 = FogSubagentB3(broker=self.broker)\n",
        "        self.fog_c = FogSubagentC(broker=self.broker, use_mock=True)\n",
        "        self.cloud_d = CloudAgentD(broker=self.broker)\n",
        "        self.cloud_e = CloudAgentE(broker=self.broker, background_data=background_data)\n",
        "        self.execution_history = []\n",
        "        # Store separate thresholds for consensus, B1, and B2\n",
        "        self.threshold_consensus = None\n",
        "        self.threshold_b1 = None\n",
        "        self.threshold_b2 = None\n",
        "\n",
        "    def train(self, X_train: np.ndarray, RUL_train: np.ndarray = None):\n",
        "        print(f\"Training {self.name} models...\")\n",
        "        X_train_data = X_train.values if hasattr(X_train, 'values') else X_train\n",
        "        self.fog_b1.train(X_train_data)\n",
        "        self.fog_b2.train(X_train_data)\n",
        "        if RUL_train is not None:\n",
        "            self.fog_b2_rul.train(X_train_data, RUL_train)\n",
        "            print(\"âœ… RUL model trained\")\n",
        "        print(\"âœ… Training complete\")\n",
        "\n",
        "    def create_predictions(self, X_test_data: np.ndarray, scores: np.ndarray, y_test_true: np.ndarray = None,\n",
        "                         is_first_iteration: bool = False, score_type: str = 'consensus',\n",
        "                         policy_threshold: float = None, adaptive_mode: bool = True):\n",
        "        rul_result = self.fog_b2_rul.execute(X_test_data)\n",
        "        scores = np.array(scores)\n",
        "\n",
        "        # Select the appropriate threshold based on score_type\n",
        "        if score_type == 'consensus':\n",
        "            threshold_attr = 'threshold_consensus'\n",
        "        elif score_type == 'b1':\n",
        "            threshold_attr = 'threshold_b1'\n",
        "        elif score_type == 'b2':\n",
        "            threshold_attr = 'threshold_b2'\n",
        "        else:\n",
        "            threshold_attr = 'threshold_consensus'\n",
        "\n",
        "        current_threshold = getattr(self, threshold_attr)\n",
        "\n",
        "        # In adaptive mode, use policy-provided threshold if available\n",
        "        if adaptive_mode and policy_threshold is not None:\n",
        "            new_threshold = policy_threshold\n",
        "            setattr(self, threshold_attr, new_threshold)\n",
        "            current_threshold = new_threshold\n",
        "        # Otherwise, calculate threshold on first iteration or if None\n",
        "        elif is_first_iteration or current_threshold is None:\n",
        "            if y_test_true is not None:\n",
        "                from sklearn.metrics import precision_recall_curve\n",
        "                prec, rec, thresholds = precision_recall_curve(y_test_true, scores)\n",
        "                f1_scores = 2 * (prec * rec) / (prec + rec + 1e-10)\n",
        "                new_threshold = thresholds[np.argmax(f1_scores)] if len(thresholds) > 0 else np.percentile(scores, 75)\n",
        "                setattr(self, threshold_attr, new_threshold)\n",
        "                print(f'   ğŸ¯ [{score_type.upper()}] Threshold calibrated: {new_threshold:.4f}')\n",
        "            else:\n",
        "                new_threshold = np.percentile(scores, 75)\n",
        "                setattr(self, threshold_attr, new_threshold)\n",
        "                print(f'   ğŸ¯ [{score_type.upper()}] Threshold set to 75th percentile: {new_threshold:.4f}')\n",
        "\n",
        "            current_threshold = new_threshold\n",
        "\n",
        "        # Use the stored threshold for predictions\n",
        "        adjusted_predictions = np.where(scores >= current_threshold, 1, 0)\n",
        "        return adjusted_predictions, rul_result, scores\n",
        "\n",
        "\n",
        "    def evaluation_metrics(self, y_test_true: np.ndarray, adjusted_predictions: np.ndarray, scores: np.ndarray):\n",
        "        metrics = {\n",
        "            'accuracy': float(accuracy_score(y_test_true, adjusted_predictions)),\n",
        "            'precision': float(precision_score(y_test_true, adjusted_predictions, zero_division=0)),\n",
        "            'recall': float(recall_score(y_test_true, adjusted_predictions, zero_division=0)),\n",
        "            'f1': float(f1_score(y_test_true, adjusted_predictions, zero_division=0)),\n",
        "            'roc_auc': float(roc_auc_score(y_test_true, scores) if len(np.unique(y_test_true)) > 1 else 0.5),\n",
        "        }\n",
        "        return metrics\n",
        "\n",
        "    def execute(self, X_test: np.ndarray, y_test_true: np.ndarray, y_test_rul: np.ndarray = None, iteration: int = 1, adaptive_mode: bool = True) -> Dict[str, Any]:\n",
        "        X_test_data = X_test.values if hasattr(X_test, 'values') else X_test\n",
        "        exec_start = time.time()\n",
        "        print(f'\\nSEMAS Iteration {iteration}')\n",
        "\n",
        "        # Get policy updates from knowledge graph (for iterations > 1)\n",
        "        policy_thresholds = {}\n",
        "        policy_contamination = None\n",
        "        if iteration > 1 and adaptive_mode:\n",
        "            kg_msg = self.broker.get_latest(MQTT_CONFIG['topics']['knowledge_graph'])\n",
        "            if kg_msg:\n",
        "                policy_thresholds['consensus'] = kg_msg.get('payload', {}).get('threshold_consensus')\n",
        "                policy_thresholds['b1'] = kg_msg.get('payload', {}).get('threshold_b1')\n",
        "                policy_thresholds['b2'] = kg_msg.get('payload', {}).get('threshold_b2')\n",
        "                policy_contamination = kg_msg.get('payload', {}).get('contamination_rate')\n",
        "\n",
        "                # Apply contamination update to B1 model\n",
        "                if policy_contamination is not None:\n",
        "                    self.fog_b1.update_contamination(policy_contamination)\n",
        "\n",
        "        self.edge_a1.execute(X_test_data)\n",
        "        self.edge_a2.execute(X_test_data)\n",
        "        b1_result = self.fog_b1.execute(X_test_data)\n",
        "        b2_result = self.fog_b2.execute(X_test_data)\n",
        "        b3_result = self.fog_b3.execute()\n",
        "        c_result = self.fog_c.execute()\n",
        "        consensus_scores = None\n",
        "        scores_b1_norm = None\n",
        "        scores_b2_norm = None\n",
        "        if b3_result:\n",
        "            consensus_scores = np.array(b3_result['consensus_scores'])\n",
        "            scores_b1_norm = np.array(b3_result['scores_b1_norm'])\n",
        "            scores_b2_norm = np.array(b3_result['scores_b2_norm'])\n",
        "            predict_exec_start_time = time.time()\n",
        "            adjusted_predictions, rul_result, consensus_array = self.create_predictions(\n",
        "                X_test_data, consensus_scores, y_test_true,\n",
        "                is_first_iteration=(iteration == 1), score_type='consensus',\n",
        "                policy_threshold=policy_thresholds.get('consensus'), adaptive_mode=adaptive_mode)\n",
        "            predict_exec_end_time = time.time()\n",
        "            predict_exec_time = predict_exec_end_time - predict_exec_start_time\n",
        "            adjusted_predictions_b1, rul_result_b1, scores_b1_norm = self.create_predictions(\n",
        "                X_test_data, scores_b1_norm, y_test_true,\n",
        "                is_first_iteration=(iteration == 1), score_type='b1',\n",
        "                policy_threshold=policy_thresholds.get('b1'), adaptive_mode=adaptive_mode)\n",
        "            adjusted_predictions_b2, rul_result_b2, scores_b2_norm = self.create_predictions(\n",
        "                X_test_data, scores_b2_norm, y_test_true,\n",
        "                is_first_iteration=(iteration == 1), score_type='b2',\n",
        "                policy_threshold=policy_thresholds.get('b2'), adaptive_mode=adaptive_mode)\n",
        "        else:\n",
        "            adjusted_predictions, rul_result = None, None\n",
        "            adjusted_predictions_b1, rul_result_b1 = None, None\n",
        "            adjusted_predictions_b2, rul_result_b2 = None, None\n",
        "        if b3_result:\n",
        "            self.fog_b.detect_drift(np.array(b3_result['consensus_scores']))\n",
        "        metrics = {}\n",
        "        if adjusted_predictions is not None:\n",
        "            eval_exec_start_time = time.time()\n",
        "            metrics_consensus = self.evaluation_metrics(y_test_true, adjusted_predictions, consensus_array)\n",
        "            eval_exec_end_time = time.time()\n",
        "            eval_exec_time = eval_exec_end_time - eval_exec_start_time\n",
        "            metrics_consensus['eval_exec_time'] = eval_exec_time\n",
        "            metrics_consensus['predict_exec_time'] = predict_exec_time\n",
        "            metrics_b1 = self.evaluation_metrics(y_test_true, adjusted_predictions_b1, scores_b1_norm)\n",
        "            metrics_b2 = self.evaluation_metrics(y_test_true, adjusted_predictions_b2, scores_b2_norm)\n",
        "            if rul_result is not None and y_test_rul is not None:\n",
        "                rul_preds = np.array(rul_result['rul_predictions'])\n",
        "                y_test_rul_array = y_test_rul.values if hasattr(y_test_rul, 'values') else y_test_rul\n",
        "                metrics_consensus['rul_mae'] = float(mean_absolute_error(y_test_rul_array[:len(rul_preds)], rul_preds))\n",
        "                metrics_consensus['rul_rmse'] = float(np.sqrt(mean_squared_error(y_test_rul_array[:len(rul_preds)], rul_preds)))\n",
        "            metrics['consensus'] = metrics_consensus\n",
        "            metrics['scores_b1_norm'] = metrics_b1\n",
        "            metrics['scores_b2_norm'] = metrics_b2\n",
        "            d_result = self.cloud_d.execute(metrics)\n",
        "            e_result = self.cloud_e.execute(adjusted_predictions, consensus_array, y_test_true, iteration)\n",
        "\n",
        "        exec_end = time.time()\n",
        "        execution_time_ms = (exec_end - exec_start) * 1000\n",
        "        metrics['execution_time_ms'] = execution_time_ms\n",
        "\n",
        "        # Store policy state and thresholds in execution result\n",
        "        execution_result = {\n",
        "            'iteration': iteration,\n",
        "            'timestamp': datetime.now().isoformat(),\n",
        "            'metrics': metrics,\n",
        "            'metrics_b1': metrics_b1,\n",
        "            'metrics_b2': metrics_b2,\n",
        "            'agent': self.name,\n",
        "            'execution_time_ms': execution_time_ms,\n",
        "            'policy_state': {\n",
        "                'contamination': policy_contamination if policy_contamination else self.fog_b1.contamination,\n",
        "                'threshold_consensus': self.threshold_consensus,\n",
        "                'threshold_b1': self.threshold_b1,\n",
        "                'threshold_b2': self.threshold_b2\n",
        "            }\n",
        "        }\n",
        "        self.publish(MQTT_CONFIG['topics']['monitoring_logs'], execution_result)\n",
        "\n",
        "        self.execution_history.append(execution_result)\n",
        "        return execution_result\n",
        "\n",
        "print(\"âœ… SEMAS COMPLETE System defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2gDFXS2Cr1Z",
        "outputId": "65d29c0d-2646-4a99-b4cc-3a87338620cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "ğŸš€ RUNNING SEMAS-COMPLETE ON BOILER DATASET\n",
            "================================================================================\n",
            "\n",
            "ğŸ“Š Dataset: Boiler Emulator\n",
            "================================================================================\n",
            "Training SEMAS_COMPLETE models...\n",
            "   âœ… B1 trained with contamination=0.3200\n",
            "âœ… RUL model trained\n",
            "âœ… Training complete\n",
            "\n",
            "SEMAS Iteration 1\n",
            "weights None\n",
            "w1: 0.4; w2: 0.6\n",
            "top 5 consensus_scores: [0.26434306 0.31967432 0.28382108 0.31092658 0.08892104]\n",
            "   ğŸ¯ [CONSENSUS] Threshold calibrated: 0.2439\n",
            "   ğŸ¯ [B1] Threshold calibrated: 0.6073\n",
            "   ğŸ¯ [B2] Threshold calibrated: 0.0000\n",
            "   ğŸ§  Policy Update: f1_consensus=0.5341, f1_b1=0.5341, f1_b2=0.4883\n",
            "   ğŸ§  Weights: w1: 0.4000 â†’ 0.4096\n",
            "   ğŸ§  Contamination: 0.3200 â†’ 0.3400 (Î”=0.0200)\n",
            "   ğŸ§  Threshold deltas: consensus: 0.0500, b1: 0.0250, b2: 0.0250\n",
            "\n",
            "   ğŸ“ˆ Iteration 1 Metrics:\n",
            "-----------\n",
            "metric: consensus\n",
            "      metric_values: {'accuracy': 0.530608504398827, 'precision': 0.3928284720363928, 'recall': 0.8340909090909091, 'f1': 0.5341095142805167, 'roc_auc': 0.6694860537190083, 'eval_exec_time': 0.01642465591430664, 'predict_exec_time': 1.4065380096435547, 'rul_mae': 31.32182502746582, 'rul_rmse': 37.60182627037133}\n",
            "-----------\n",
            "metric: scores_b1_norm\n",
            "      metric_values: {'accuracy': 0.5289589442815249, 'precision': 0.39217252396166136, 'recall': 0.8369318181818182, 'f1': 0.5340826686004351, 'roc_auc': 0.6695338633903973}\n",
            "-----------\n",
            "metric: scores_b2_norm\n",
            "      metric_values: {'accuracy': 0.32404692082111436, 'precision': 0.32305433186490456, 'recall': 1.0, 'f1': 0.48834628190899, 'roc_auc': 0.4892361367325856}\n",
            "-----------\n",
            "metric: execution_time_ms\n",
            "      metric_values: 4218.084096908569\n",
            "\n",
            "SEMAS Iteration 2\n",
            "   ğŸ”„ B1 Retraining: contamination 0.3200 â†’ 0.3400\n",
            "weights {'timestamp': '2026-01-12T06:57:59.174813', 'topic': 'graph/sync', 'payload': {'w1': 0.40957363866914454, 'w2': 0.5904263613308555, 'contamination_rate': 0.34, 'threshold_delta_consensus': 0.05, 'threshold_delta_b1': 0.025, 'threshold_delta_b2': 0.025, 'data_quantity': 1}, 'qos': 1}\n",
            "w1: 0.40957363866914454; w2: 0.5904263613308555\n",
            "top 5 consensus_scores: [0.27066293 0.32731848 0.29060713 0.31836138 0.09104234]\n",
            "   ğŸ§  Policy Update: f1_consensus=0.5325, f1_b1=0.5341, f1_b2=0.4883\n",
            "   ğŸ§  Weights: w1: 0.4096 â†’ 0.4191\n",
            "   ğŸ§  Contamination: 0.3400 â†’ 0.3600 (Î”=0.0200)\n",
            "   ğŸ§  Threshold deltas: consensus: 0.0500, b1: 0.0250, b2: 0.0250\n",
            "\n",
            "   ğŸ“ˆ Iteration 2 Metrics:\n",
            "-----------\n",
            "metric: consensus\n",
            "      metric_values: {'accuracy': 0.5153958944281525, 'precision': 0.38655030800821355, 'recall': 0.8556818181818182, 'f1': 0.5325318246110325, 'roc_auc': 0.6694883596517119, 'eval_exec_time': 0.011082887649536133, 'predict_exec_time': 0.736774206161499, 'rul_mae': 31.32182502746582, 'rul_rmse': 37.60182627037133}\n",
            "-----------\n",
            "metric: scores_b1_norm\n",
            "      metric_values: {'accuracy': 0.5289589442815249, 'precision': 0.39217252396166136, 'recall': 0.8369318181818182, 'f1': 0.5340826686004351, 'roc_auc': 0.6695338633903973}\n",
            "-----------\n",
            "metric: scores_b2_norm\n",
            "      metric_values: {'accuracy': 0.32404692082111436, 'precision': 0.32305433186490456, 'recall': 1.0, 'f1': 0.48834628190899, 'roc_auc': 0.4892361367325856}\n",
            "-----------\n",
            "metric: execution_time_ms\n",
            "      metric_values: 4074.5346546173096\n",
            "\n",
            "SEMAS Iteration 3\n",
            "   ğŸ”„ B1 Retraining: contamination 0.3400 â†’ 0.3600\n",
            "weights {'timestamp': '2026-01-12T06:58:03.254583', 'topic': 'graph/sync', 'payload': {'w1': 0.41914727733828905, 'w2': 0.580852722661711, 'contamination_rate': 0.36000000000000004, 'threshold_delta_consensus': 0.05, 'threshold_delta_b1': 0.025, 'threshold_delta_b2': 0.025, 'data_quantity': 2}, 'qos': 1}\n",
            "w1: 0.41914727733828905; w2: 0.580852722661711\n",
            "top 5 consensus_scores: [0.27698279 0.33496265 0.29739318 0.32579618 0.09316363]\n",
            "   ğŸ§  Policy Update: f1_consensus=0.5277, f1_b1=0.5341, f1_b2=0.4883\n",
            "   ğŸ§  Weights: w1: 0.4191 â†’ 0.4287\n",
            "   ğŸ§  Contamination: 0.3600 â†’ 0.3800 (Î”=0.0200)\n",
            "   ğŸ§  Threshold deltas: consensus: 0.0500, b1: 0.0250, b2: 0.0250\n",
            "\n",
            "   ğŸ“ˆ Iteration 3 Metrics:\n",
            "-----------\n",
            "metric: consensus\n",
            "      metric_values: {'accuracy': 0.49908357771260997, 'precision': 0.37919046436553266, 'recall': 0.8676136363636363, 'f1': 0.527734577501296, 'roc_auc': 0.6694895894824872, 'eval_exec_time': 0.010833024978637695, 'predict_exec_time': 1.186018466949463, 'rul_mae': 31.32182502746582, 'rul_rmse': 37.60182627037133}\n",
            "-----------\n",
            "metric: scores_b1_norm\n",
            "      metric_values: {'accuracy': 0.5289589442815249, 'precision': 0.39217252396166136, 'recall': 0.8369318181818182, 'f1': 0.5340826686004351, 'roc_auc': 0.6695338633903973}\n",
            "-----------\n",
            "metric: scores_b2_norm\n",
            "      metric_values: {'accuracy': 0.32404692082111436, 'precision': 0.32305433186490456, 'recall': 1.0, 'f1': 0.48834628190899, 'roc_auc': 0.4892361367325856}\n",
            "-----------\n",
            "metric: execution_time_ms\n",
            "      metric_values: 5005.110025405884\n",
            "\n",
            "================================================================================\n",
            "ğŸš€ RUNNING SEMAS-COMPLETE ON WIND TURBINE DATASET (FIXED LABELS)\n",
            "================================================================================\n",
            "\n",
            "ğŸ“Š Dataset: Wind Turbine\n",
            "================================================================================\n",
            "Training SEMAS_COMPLETE models...\n",
            "   âœ… B1 trained with contamination=0.3200\n",
            "âœ… RUL model trained\n",
            "âœ… Training complete\n",
            "\n",
            "SEMAS Iteration 1\n",
            "weights {'timestamp': '2026-01-12T06:58:08.259407', 'topic': 'graph/sync', 'payload': {'w1': 0.42872091600743356, 'w2': 0.5712790839925664, 'contamination_rate': 0.38000000000000006, 'threshold_delta_consensus': 0.05, 'threshold_delta_b1': 0.025, 'threshold_delta_b2': 0.025, 'data_quantity': 3}, 'qos': 1}\n",
            "w1: 0.42872091600743356; w2: 0.5712790839925664\n",
            "top 5 consensus_scores: [0.43736357 0.22142997 0.26781038 0.4014392  0.29036524]\n",
            "   ğŸ¯ [CONSENSUS] Threshold calibrated: 0.0511\n",
            "   ğŸ¯ [B1] Threshold calibrated: 0.0000\n",
            "   ğŸ¯ [B2] Threshold calibrated: 0.0000\n",
            "   ğŸ§  Policy Update: f1_consensus=0.9606, f1_b1=0.9606, f1_b2=0.9606\n",
            "   ğŸ§  Weights: w1: 0.4287 â†’ 0.4337\n",
            "   ğŸ§  Contamination: 0.3800 â†’ 0.3600 (Î”=-0.0200)\n",
            "   ğŸ§  Threshold deltas: consensus: 0.0000, b1: 0.0000, b2: 0.0000\n",
            "\n",
            "   ğŸ“ˆ Iteration 1 Metrics:\n",
            "-----------\n",
            "metric: consensus\n",
            "      metric_values: {'accuracy': 0.9242424242424242, 'precision': 0.9242424242424242, 'recall': 1.0, 'f1': 0.9606299212598425, 'roc_auc': 0.5377049180327869, 'eval_exec_time': 0.00809335708618164, 'predict_exec_time': 0.680689811706543, 'rul_mae': 7.201471328735352, 'rul_rmse': 13.091529239068398}\n",
            "-----------\n",
            "metric: scores_b1_norm\n",
            "      metric_values: {'accuracy': 0.9242424242424242, 'precision': 0.9242424242424242, 'recall': 1.0, 'f1': 0.9606299212598425, 'roc_auc': 0.7409836065573769}\n",
            "-----------\n",
            "metric: scores_b2_norm\n",
            "      metric_values: {'accuracy': 0.9242424242424242, 'precision': 0.9242424242424242, 'recall': 1.0, 'f1': 0.9606299212598425, 'roc_auc': 0.42131147540983604}\n",
            "-----------\n",
            "metric: execution_time_ms\n",
            "      metric_values: 1288.4728908538818\n",
            "\n",
            "SEMAS Iteration 2\n",
            "   ğŸ”„ B1 Retraining: contamination 0.3200 â†’ 0.3600\n",
            "weights {'timestamp': '2026-01-12T06:58:20.911938', 'topic': 'graph/sync', 'payload': {'w1': 0.43372091600743357, 'w2': 0.5662790839925664, 'contamination_rate': 0.36000000000000004, 'threshold_delta_consensus': 0.0, 'threshold_delta_b1': 0.0, 'threshold_delta_b2': 0.0, 'data_quantity': 1}, 'qos': 1}\n",
            "w1: 0.43372091600743357; w2: 0.5662790839925664\n",
            "top 5 consensus_scores: [0.44195502 0.22350307 0.27042439 0.40561167 0.2932423 ]\n",
            "   ğŸ§  Policy Update: f1_consensus=0.9524, f1_b1=0.9606, f1_b2=0.9606\n",
            "   ğŸ§  Weights: w1: 0.4337 â†’ 0.4387\n",
            "   ğŸ§  Contamination: 0.3600 â†’ 0.3400 (Î”=-0.0200)\n",
            "   ğŸ§  Threshold deltas: consensus: 0.0000, b1: 0.0000, b2: 0.0000\n",
            "\n",
            "   ğŸ“ˆ Iteration 2 Metrics:\n",
            "-----------\n",
            "metric: consensus\n",
            "      metric_values: {'accuracy': 0.9090909090909091, 'precision': 0.9230769230769231, 'recall': 0.9836065573770492, 'f1': 0.9523809523809523, 'roc_auc': 0.5442622950819671, 'eval_exec_time': 0.009280681610107422, 'predict_exec_time': 0.09766340255737305, 'rul_mae': 7.201471328735352, 'rul_rmse': 13.091529239068398}\n",
            "-----------\n",
            "metric: scores_b1_norm\n",
            "      metric_values: {'accuracy': 0.9242424242424242, 'precision': 0.9242424242424242, 'recall': 1.0, 'f1': 0.9606299212598425, 'roc_auc': 0.7409836065573769}\n",
            "-----------\n",
            "metric: scores_b2_norm\n",
            "      metric_values: {'accuracy': 0.9242424242424242, 'precision': 0.9242424242424242, 'recall': 1.0, 'f1': 0.9606299212598425, 'roc_auc': 0.42131147540983604}\n",
            "-----------\n",
            "metric: execution_time_ms\n",
            "      metric_values: 889.4119262695312\n",
            "\n",
            "SEMAS Iteration 3\n",
            "   ğŸ”„ B1 Retraining: contamination 0.3600 â†’ 0.3400\n",
            "weights {'timestamp': '2026-01-12T06:58:21.797627', 'topic': 'graph/sync', 'payload': {'w1': 0.43872091600743357, 'w2': 0.5612790839925664, 'contamination_rate': 0.34, 'threshold_delta_consensus': 0.0, 'threshold_delta_b1': 0.0, 'threshold_delta_b2': 0.0, 'data_quantity': 2}, 'qos': 1}\n",
            "w1: 0.43872091600743357; w2: 0.5612790839925664\n",
            "top 5 consensus_scores: [0.44654646 0.22557617 0.27303841 0.40978415 0.29611936]\n",
            "   ğŸ§  Policy Update: f1_consensus=0.9524, f1_b1=0.9606, f1_b2=0.9606\n",
            "   ğŸ§  Weights: w1: 0.4387 â†’ 0.4437\n",
            "   ğŸ§  Contamination: 0.3400 â†’ 0.3200 (Î”=-0.0200)\n",
            "   ğŸ§  Threshold deltas: consensus: 0.0000, b1: 0.0000, b2: 0.0000\n",
            "\n",
            "   ğŸ“ˆ Iteration 3 Metrics:\n",
            "-----------\n",
            "metric: consensus\n",
            "      metric_values: {'accuracy': 0.9090909090909091, 'precision': 0.9230769230769231, 'recall': 0.9836065573770492, 'f1': 0.9523809523809523, 'roc_auc': 0.5508196721311476, 'eval_exec_time': 0.008085966110229492, 'predict_exec_time': 0.08896255493164062, 'rul_mae': 7.201471328735352, 'rul_rmse': 13.091529239068398}\n",
            "-----------\n",
            "metric: scores_b1_norm\n",
            "      metric_values: {'accuracy': 0.9242424242424242, 'precision': 0.9242424242424242, 'recall': 1.0, 'f1': 0.9606299212598425, 'roc_auc': 0.7409836065573769}\n",
            "-----------\n",
            "metric: scores_b2_norm\n",
            "      metric_values: {'accuracy': 0.9242424242424242, 'precision': 0.9242424242424242, 'recall': 1.0, 'f1': 0.9606299212598425, 'roc_auc': 0.42131147540983604}\n",
            "-----------\n",
            "metric: execution_time_ms\n",
            "      metric_values: 833.9822292327881\n"
          ]
        }
      ],
      "source": [
        "def run_semas_complete(dataset_key: str, dataset_config: Dict[str, Any], num_iterations: int = 3):\n",
        "    print(f'\\nğŸ“Š Dataset: {dataset_config[\"name\"]}')\n",
        "    print('=' * 80)\n",
        "    semas = SEMAS_COMPLETE(broker=message_broker, background_data=dataset_config['X_train'].values if hasattr(dataset_config['X_train'], 'values') else dataset_config['X_train'])\n",
        "    semas.train(dataset_config['X_train'], dataset_config.get('y_test_rul'))\n",
        "    X_test = dataset_config['X_test']\n",
        "    y_test_binary = dataset_config['y_test_anomaly'].values if hasattr(dataset_config['y_test_anomaly'], 'values') else dataset_config['y_test_anomaly']\n",
        "    y_test_rul = dataset_config.get('y_test_rul')\n",
        "    all_iterations = []\n",
        "    for iteration in range(num_iterations):\n",
        "        result = semas.execute(X_test, y_test_binary, y_test_rul, iteration=iteration + 1)\n",
        "        if result['metrics']:\n",
        "            print(f'\\n   ğŸ“ˆ Iteration {iteration + 1} Metrics:')\n",
        "            for (metric, metric_values) in result['metrics'].items():\n",
        "                print('-----------')\n",
        "                print(f'metric: {metric}')\n",
        "                print(f'      metric_values: {metric_values}')\n",
        "        all_iterations.append(result)\n",
        "    return {'dataset': dataset_key, 'iterations': all_iterations, 'semas_instance': semas}\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print('ğŸš€ RUNNING SEMAS-COMPLETE ON BOILER DATASET')\n",
        "print(\"=\"*80)\n",
        "semas_boiler = run_semas_complete('boiler', CONFIG['datasets']['boiler'], num_iterations=3)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print('ğŸš€ RUNNING SEMAS-COMPLETE ON WIND TURBINE DATASET (FIXED LABELS)')\n",
        "print(\"=\"*80)\n",
        "semas_turbine = run_semas_complete('wind_turbine', CONFIG['datasets']['wind_turbine'], num_iterations=3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BhI1WFYKCr1Z",
        "outputId": "9b8f2cf6-df88-48e3-bfe5-ea3ed450e507"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "ğŸ“Š METRICS ANALYSIS\n",
            "================================================================================\n",
            "\n",
            "BOILER DATASET\n",
            "--------------------------------------------------------------------------------\n",
            " accuracy  precision   recall       f1  roc_auc  eval_exec_time  predict_exec_time   rul_mae  rul_rmse  iteration\n",
            " 0.530609   0.392828 0.834091 0.534110 0.669486        0.016425           1.406538 31.321825 37.601826          1\n",
            " 0.515396   0.386550 0.855682 0.532532 0.669488        0.011083           0.736774 31.321825 37.601826          2\n",
            " 0.499084   0.379190 0.867614 0.527735 0.669490        0.010833           1.186018 31.321825 37.601826          3\n",
            "\n",
            "Average F1: 0.5315, Precision: 0.3862, Recall: 0.8525\n",
            "\n",
            "WIND_TURBINE DATASET\n",
            "--------------------------------------------------------------------------------\n",
            " accuracy  precision   recall       f1  roc_auc  eval_exec_time  predict_exec_time  rul_mae  rul_rmse  iteration\n",
            " 0.924242   0.924242 1.000000 0.960630 0.537705        0.008093           0.680690 7.201471 13.091529          1\n",
            " 0.909091   0.923077 0.983607 0.952381 0.544262        0.009281           0.097663 7.201471 13.091529          2\n",
            " 0.909091   0.923077 0.983607 0.952381 0.550820        0.008086           0.088963 7.201471 13.091529          3\n",
            "\n",
            "Average F1: 0.9551, Precision: 0.9235, Recall: 0.9891\n"
          ]
        }
      ],
      "source": [
        "print('\\n' + '=' * 80)\n",
        "print('ğŸ“Š METRICS ANALYSIS')\n",
        "print('=' * 80)\n",
        "\n",
        "def analyze_results(semas_results, dataset_name):\n",
        "    print(f'\\n{dataset_name.upper()} DATASET')\n",
        "    print('-' * 80)\n",
        "    iterations = semas_results['iterations']\n",
        "    all_metrics = []\n",
        "    for i, iter_result in enumerate(iterations):\n",
        "        if iter_result['metrics']['consensus']:\n",
        "            metrics_copy = iter_result['metrics']['consensus'].copy()\n",
        "            metrics_copy['iteration'] = i + 1\n",
        "            all_metrics.append(metrics_copy)\n",
        "    if all_metrics:\n",
        "        metrics_df = pd.DataFrame(all_metrics)\n",
        "        print(metrics_df.to_string(index=False))\n",
        "        print(f'\\nAverage F1: {metrics_df[\"f1\"].mean():.4f}, Precision: {metrics_df[\"precision\"].mean():.4f}, Recall: {metrics_df[\"recall\"].mean():.4f}')\n",
        "\n",
        "analyze_results(semas_boiler, 'boiler')\n",
        "analyze_results(semas_turbine, 'wind_turbine')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SgQKTm2T6-oD",
        "outputId": "b08a96ad-44c7-4d84-cada-2e8c4c4ec3cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "ğŸ“¦ COLLECTING SEMAS PERFORMANCE RESULTS\n",
            "================================================================================\n",
            "\n",
            "âœ… BOILER DATASET RESULTS:\n",
            "   Avg F1: 0.5315 Â± 0.0027\n",
            "   Avg Precision: 0.3862\n",
            "   Avg Recall: 0.8525\n",
            "   Avg Latency: 4432.58ms\n",
            "   F1 Trajectory: ['0.5341', '0.5325', '0.5277']\n",
            "   F1 Improvement: -0.0064\n",
            "\n",
            "âœ… WIND TURBINE DATASET RESULTS:\n",
            "   Avg F1: 0.9551 Â± 0.0039\n",
            "   Avg Precision: 0.9235\n",
            "   Avg Recall: 0.9891\n",
            "   Avg Latency: 1003.96ms\n",
            "   F1 Trajectory: ['0.9606', '0.9524', '0.9524']\n",
            "   F1 Improvement: -0.0082\n",
            "\n",
            "ğŸ’¾ Results saved to: semas_performance_boiler.pkl, semas_performance_turbine.pkl\n",
            "\n",
            "================================================================================\n",
            "âœ… PERFORMANCE COLLECTION COMPLETE\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# PERFORMANCE RESULTS COLLECTION FOR CROSS-PIPELINE COMPARISON\n",
        "# ============================================================================\n",
        "import pickle\n",
        "from datetime import datetime\n",
        "\n",
        "def collect_performance_results(semas_results, dataset_name, system_name='SEMAS'):\n",
        "    \"\"\"\n",
        "    Collect comprehensive performance metrics for comparison across pipelines\n",
        "    Enhanced version with policy evolution tracking (w1/w2 weights)\n",
        "    \"\"\"\n",
        "    iterations_data = semas_results['iterations']\n",
        "    semas_instance = semas_results.get('semas_instance')\n",
        "\n",
        "    # Extract metrics from each iteration\n",
        "    performance_log = {\n",
        "        'system': system_name,\n",
        "        'dataset': dataset_name,\n",
        "        'timestamp': datetime.now().isoformat(),\n",
        "        'num_iterations': len(iterations_data),\n",
        "        'iterations': []\n",
        "    }\n",
        "\n",
        "    for idx, iter_result in enumerate(iterations_data, 1):\n",
        "        iteration_metrics = {\n",
        "            'iteration': idx,\n",
        "            'metrics': {},\n",
        "            'latency_ms': iter_result.get('execution_time_ms', 0),\n",
        "            'layer_times': {}\n",
        "        }\n",
        "\n",
        "        # Extract layer execution times if available\n",
        "        if iter_result.get('metrics') and iter_result['metrics'].get('consensus'):\n",
        "            consensus = iter_result['metrics']['consensus']\n",
        "            iteration_metrics['layer_times'] = {\n",
        "                'eval_time': consensus.get('eval_exec_time', 0) * 1000,  # Convert to ms\n",
        "                'predict_time': consensus.get('predict_exec_time', 0) * 1000\n",
        "            }\n",
        "\n",
        "        # Extract consensus metrics\n",
        "        if iter_result.get('metrics') and iter_result['metrics'].get('consensus'):\n",
        "            consensus = iter_result['metrics']['consensus']\n",
        "            iteration_metrics['metrics'] = {\n",
        "                'accuracy': consensus.get('accuracy', 0),\n",
        "                'precision': consensus.get('precision', 0),\n",
        "                'recall': consensus.get('recall', 0),\n",
        "                'f1': consensus.get('f1', 0),\n",
        "                'roc_auc': consensus.get('roc_auc', 0),\n",
        "                'mcc': consensus.get('mcc', 0),\n",
        "                'rul_mae': consensus.get('rul_mae', 0),\n",
        "                'rul_rmse': consensus.get('rul_rmse', 0)\n",
        "            }\n",
        "\n",
        "        performance_log['iterations'].append(iteration_metrics)\n",
        "\n",
        "    # Calculate aggregated statistics\n",
        "    if performance_log['iterations']:\n",
        "        all_f1 = [it['metrics'].get('f1', 0) for it in performance_log['iterations']]\n",
        "        all_precision = [it['metrics'].get('precision', 0) for it in performance_log['iterations']]\n",
        "        all_recall = [it['metrics'].get('recall', 0) for it in performance_log['iterations']]\n",
        "        all_latency = [it['latency_ms'] for it in performance_log['iterations']]\n",
        "\n",
        "        performance_log['summary'] = {\n",
        "            'avg_f1': np.mean(all_f1),\n",
        "            'std_f1': np.std(all_f1),\n",
        "            'avg_precision': np.mean(all_precision),\n",
        "            'avg_recall': np.mean(all_recall),\n",
        "            'avg_latency_ms': np.mean(all_latency),\n",
        "            'f1_trajectory': all_f1,\n",
        "            'f1_improvement': all_f1[-1] - all_f1[0] if len(all_f1) > 1 else 0\n",
        "        }\n",
        "\n",
        "        # Extract policy evolution (w1/w2 weights from CloudAgentD)\n",
        "        if semas_instance and hasattr(semas_instance, 'cloud_d'):\n",
        "            cloud_d = semas_instance.cloud_d\n",
        "\n",
        "            # Get weight history from knowledge graph messages\n",
        "            weight_history = []\n",
        "            kg_messages = message_broker.get_all(MQTT_CONFIG['topics']['knowledge_graph'])\n",
        "\n",
        "            for msg in kg_messages:\n",
        "                if 'payload' in msg:\n",
        "                    payload = msg['payload']\n",
        "                    weight_history.append({\n",
        "                        'w1': payload.get('w1', 0.4),\n",
        "                        'w2': payload.get('w2', 0.6),\n",
        "                        'data_quantity': payload.get('data_quantity', 0)\n",
        "                    })\n",
        "\n",
        "            if weight_history:\n",
        "                performance_log['policy_evolution'] = {\n",
        "                    'initial_w1': weight_history[0]['w1'],\n",
        "                    'final_w1': weight_history[-1]['w1'],\n",
        "                    'initial_w2': weight_history[0]['w2'],\n",
        "                    'final_w2': weight_history[-1]['w2'],\n",
        "                    'weight_changes': len(weight_history) - 1,\n",
        "                    'weight_history': weight_history\n",
        "                }\n",
        "\n",
        "    return performance_log\n",
        "\n",
        "# Collect performance results for both datasets\n",
        "print('\\n' + '='*80)\n",
        "print('ğŸ“¦ COLLECTING SEMAS PERFORMANCE RESULTS')\n",
        "print('='*80)\n",
        "\n",
        "semas_performance_boiler = collect_performance_results(semas_boiler, 'boiler', 'SEMAS')\n",
        "semas_performance_turbine = collect_performance_results(semas_turbine, 'wind_turbine', 'SEMAS')\n",
        "\n",
        "print('\\nâœ… BOILER DATASET RESULTS:')\n",
        "print(f'   Avg F1: {semas_performance_boiler[\"summary\"][\"avg_f1\"]:.4f} Â± {semas_performance_boiler[\"summary\"][\"std_f1\"]:.4f}')\n",
        "print(f'   Avg Precision: {semas_performance_boiler[\"summary\"][\"avg_precision\"]:.4f}')\n",
        "print(f'   Avg Recall: {semas_performance_boiler[\"summary\"][\"avg_recall\"]:.4f}')\n",
        "print(f'   Avg Latency: {semas_performance_boiler[\"summary\"][\"avg_latency_ms\"]:.2f}ms')\n",
        "print(f'   F1 Trajectory: {[f\"{x:.4f}\" for x in semas_performance_boiler[\"summary\"][\"f1_trajectory\"]]}')\n",
        "print(f'   F1 Improvement: {semas_performance_boiler[\"summary\"][\"f1_improvement\"]:.4f}')\n",
        "\n",
        "print('\\nâœ… WIND TURBINE DATASET RESULTS:')\n",
        "print(f'   Avg F1: {semas_performance_turbine[\"summary\"][\"avg_f1\"]:.4f} Â± {semas_performance_turbine[\"summary\"][\"std_f1\"]:.4f}')\n",
        "print(f'   Avg Precision: {semas_performance_turbine[\"summary\"][\"avg_precision\"]:.4f}')\n",
        "print(f'   Avg Recall: {semas_performance_turbine[\"summary\"][\"avg_recall\"]:.4f}')\n",
        "print(f'   Avg Latency: {semas_performance_turbine[\"summary\"][\"avg_latency_ms\"]:.2f}ms')\n",
        "print(f'   F1 Trajectory: {[f\"{x:.4f}\" for x in semas_performance_turbine[\"summary\"][\"f1_trajectory\"]]}')\n",
        "print(f'   F1 Improvement: {semas_performance_turbine[\"summary\"][\"f1_improvement\"]:.4f}')\n",
        "\n",
        "# Save results to pickle files for cross-pipeline comparison\n",
        "try:\n",
        "    with open('semas_performance_boiler.pkl', 'wb') as f:\n",
        "        pickle.dump(semas_performance_boiler, f)\n",
        "    with open('semas_performance_turbine.pkl', 'wb') as f:\n",
        "        pickle.dump(semas_performance_turbine, f)\n",
        "    print('\\nğŸ’¾ Results saved to: semas_performance_boiler.pkl, semas_performance_turbine.pkl')\n",
        "except Exception as e:\n",
        "    print(f'\\nâš ï¸  Could not save pickle files: {e}')\n",
        "\n",
        "print('\\n' + '='*80)\n",
        "print('âœ… PERFORMANCE COLLECTION COMPLETE')\n",
        "print('='*80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dcpgOvd1ST0c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9WSiprH_ST0c"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
